* ZuseNEAT                                                         :TOC_5_gh:
  - [[#overview][Overview]]
    - [[#why-zuse][Why Zuse?]]
    - [[#why-c20][Why C++20?]]
    - [[#why-clang][Why Clang?]]
    - [[#port-from-rubyneat-et-al][Port from RubyNEAT, et al]]
  - [[#architectural-notes][Architectural Notes]]
    - [[#populations-and-fluidity][Populations and Fluidity]]
    - [[#overall-layout][Overall Layout]]
    - [[#evaluation-interfaces-to-cppns-etc][Evaluation, Interfaces to CPPNs, etc.]]
      - [[#es-iterated-hyperneat-and-cppns][ES Iterated HyperNEAT and CPPNs]]
      - [[#interfaces-for-other-languages][Interfaces for other languages]]
      - [[#resultant-stand-alone-outputs-not-requiring-the-engine][Resultant stand-alone outputs not requiring the engine]]
      - [[#evaluations-and-gpus][Evaluations and GPUs]]
      - [[#independent-vs-tribal-evolution][Independent vs. Tribal evolution]]
      - [[#hyperneat-allows-us-to-sample-large-vectors-or-matricies][HyperNEAT allows us to sample large vectors or matricies]]
    - [[#critters-and-the-shared-critter-loader][Critters and the Shared Critter Loader]]
  - [[#building-and-installing][Building and Installing]]
    - [[#building-requirements][Building Requirements]]
    - [[#how-to-build][How to build]]
  - [[#to-jupiter-and-beyond-the-infinite][To Jupiter and beyond the Infinite]]
    - [[#numenta][Numenta]]
      - [[#htm-encoding-sparsity-and-spatial-pooling][HTM, Encoding, Sparsity, and Spatial Pooling]]
      - [[#pyramidial-neurons][Pyramidial Neurons]]
      - [[#neocortex-columns][Neocortex Columns]]
        - [[#htm-like-evolution-to-colums][HTM-like evolution to colums]]

** Overview
   This is a / will be my 2nd implementation of the
   NEAT Algorithm, and this time it's in C++20. My
   prior NEAT implementation was in pure Ruby, and was
   more a proof of concept than anything. ZuseNEAT
   shall be much more powerful.

   I have much planned here, but no promises. You'll
   know when the coolness is implemented.
*** Why Zuse?
    Konrad Zuse invented the world's first digital
    computer. This man never got the recognition he
    deserved. As well, his great work was overshadowed
    by der Weltkrieg To find out more,

    https://www.quora.com/Who-is-the-greatest-person-that-history-has-forgotten/answer/Fred-Mitchell-5?share=c89046d6&srid=pTaaZ

    Today, we have moved far beyond his humble initial
    creation, the Z1, which was entirely
    mechanical. History has almost forgotten him, and
    so I name this NEAT implementation after him in
    honer of his accomplishments.

*** Why C++20?
    There are many cutting-edge features in C++20, and one may as well
    get started with them. For sure, at the time of this writing, not all
    features are in place, and there may be some small revisions.  But, with
    the way I see it, by the time this project is completed, most of the
    features will be in place in C++20 anyway.

*** Why Clang?
    We are leveraging clang primarily for its support for both the
    GPU and for LLVM. While it would be possible to do both from gcc,
    we think it would simply be easier to do in clang.

    Later, we want to be compiler-agnostic, perhaps disabling clang-specific
    features as an option. Or support both clang and gcc. We'll see as this 
    progresses.

*** Port from RubyNEAT, et al
    RubyNEAT is basically a "proof of concept", to see how NEAT works
    and to become intimately familiar with the algorithm. Now that I am,
    it is time to get serious and do it in a "real man's language".

    To honor its humble begininngs, RubyNEAT will live on as a wrapper
    for ZuseNEAT. Python support is also planned, as it is the 500kg
    gorilla in the room when it comes to data science and machine learning.

    A plugin for TensorFlow is also a possibility. But one thing at a time...

** Architectural Notes
   RAII and SOLID principles apply, where they make sense.
   I want this to be easily maintainable and understandable
   by others -- to encourage pull requests! For once this
   is begun, I anticipate that this will grow to become a 
   multi-person effort.

*** Populations and Fluidity 
    In the prior implementation of NEAT, a "population"
    was a fixed container of Critters, and all the
    Critters were rigidly evaluated and bred at each
    iteration.

    I wish to move away from that and have a more fluid
    approach, that can and will be more
    Critter-oriented, and more fluid, and will allow
    each Critter to be evaluated on its own
    thread. When it's ready to mate, it will flag a
    mating mode and then "seek" a potential mate based
    on the settings of the hyper parameters. It will
    simply linger until it can find another suitable
    critter. It could do continuous evaluation, and
    simply pause at mating time.

    Also, the more fluid approach will also make it
    easier for Critters to compete with each other!

*** Overall Layout 
    | Class       | Description                                                         |
    |-------------+---------------------------------------------------------------------|
    | Geneotype   | Basic gene                                                          |
    | Phenotype   | Basic expression of the genotype, AST or vector, etc                |
    | Critter     | General unit of evolution, the ANN collection                       |
    | Config      | Hyperparameters                                                     |
    | Coordinator | Fluid mode controller                                               |
    | Reporter    | Reports on progress, performace, etc                                |
    | Gateway     | Coordinator and Synchronizer to other ZNEAT nodes on other machines |
    |             |                                                                     |

*** Evaluation, Interfaces to CPPNs, etc.
    We have decided that the evaluation module shall be
    basically be written in C or C++, or in whatever
    language that can call C with the appropriate
    "callbacks".
    
    Ideally, we pass a vector or matrix or tensor to
    the critter, and in return we get something
    back. The same? Why not.

    So, for example, we could pass in an image as a
    matrix of floats. Maybe we'd get back a matrix of
    floats as well, which might represent the
    sharpening of the image, or some other sort of
    processing or feature extraction. Or in the case of
    sound, we pass in a Foureir vector and get
    something back. Or in the case of text, we pass in
    some sort of word vectors, and get something back
    along those lines, or something completely
    different.

**** ES Iterated HyperNEAT and CPPNs
     We come with a pre-canned suite of CPPNs, but
     allow more to be "wired in" via using a specific
     prescribed contract, thus allowing for
     extensibility.

**** Interfaces for other languages
     We wish to make this as "language-agnostic" as
     possible. If someone wants to use Ruby or Python
     or Erlang or Rust to interface with us, we should
     not care. And so language wrappers would have to
     be written.

**** Resultant stand-alone outputs not requiring the engine
     In RubyNEAT, we emitted stand-alone code. In this
     case, we emit object files generated by the LLVM
     that are linkable, and maybe even .so files that
     can be used with other languages. We can rely,
     perhaps, on CMake to make this go in a
     cross-compiler fashion to target any platform.
     
     As such, we will save either the ASTs or IRs for
     LLVM, and have a specific facility to target some
     platform. This will allow us to run the evolution
     on one type of hardware, and target the successful
     critters to something completely different.

**** Evaluations and GPUs
     We have a bit of an issue with data streams going
     accross the GPU/CPU boundaries, which can result
     in a significant slowdown. On the one hand,
     leveraging a GPU with a thousand cores might be a
     very powerful thing to do, except if most of the
     time is spent transferring data back and forth
     with the host system.

     So it may be that we, in that case, keep data
     requirements light, or put the entire problem
     space onto the GPU so that all computations and
     evaluations take place there. Is this a viable
     option? GPUs are not CPUs, so that approach will
     be rather restrictive.
     
     Better will be CPUs with large number of cores, or
     distributed systems, as in the cloud, etc.

**** Independent vs. Tribal evolution
     We want to be able to support both modalities
     where we evaluate the critters in isolation from
     each other, vs.  evaluating the critters in a
     group, or in pairs or similar sub-groupings.

     Keep in mind that there is no longer any definite
     "population" demarcation as there was with
     RubyNEAT. The population will be more sliding in
     the iterations.
     
**** HyperNEAT allows us to sample large vectors or matricies
     We do not have to have input neurons attached to
     all inputs in a vector, but we could simply take
     groups of local inputs and process them in a
     fashion. This would allow us to have variable
     scale inputs, in the Enhanced Substrate fashion.

*** Critters and the Shared Critter Loader
    Critters are generated as shared libraries that are
    dynamically loaded by shared-critter-loader.

    We need to be able to load Critters (as shared
    libraries) dymacally. The functions in the shared
    library can be called directly. See the docs in the
    URL below.
 
    The underlying C interfaces are dladdr(), dlclose(),
    dlerror(), dlopen(), dlsym(), dlvsym(). All is
    described at:
 
    https://linux.die.net/man/3/dlopen

** Building and Installing
*** Building Requirements
    We use clang 5 or later to leverage the full C++17
    specs, and also so that we can eaisly target
    multiple environments.

    We also are using googletest to run our unit tests
    and the like.

    When built, all executables will be in the root of
    the build directory.

*** How to build
    To build this project (usung Ninja):

    #+begin_src bash
    mkdir build
    cd build && cmake -GNinja .. && ninja -k3 -j8
    #+end_src

    Feel free to adjust the "-j8" parameter to reflect
    the number of cores on your build system. You can
    also leave off the "-GNinja" flag if you wish to
    use make instead.

    So, a "no frills" build would look like:

    #+begin_src bash
    mkdir build
    cd build && cmake .. && make
    #+end_src
** To Jupiter and beyond the Infinite
   ZuseNEAT is just the first tiny baby step in a number of steps I have in mind
   to create AGI. I have envisioned how to bring it about. My idea is
   at once beautiful and elegant, yet terrifying. It will be both
   easy and difficult. BUT, nature has already done it the first time.
   I will be simply taking the basic priciples of nature and extending
   them in a way that will allow up to propel forward beyond anyone's 
   wildest dreams.

   It's all in the genes. It is all in the genes. The clue is there.
   Just not the one everyone expects or would look for.

*** Numenta
    New capabilities are planned for ZuseNEAT in the future,
    inspired by the research being done at Numenta.
    https://numenta.com/

**** HTM, Encoding, Sparsity, and Spatial Pooling
     Some thought will be given to how to incorporate HTM theory into
     ZuseNEAT. What implications does this have for ES Interated HyperNEAT?
     We don't know yet.

**** Pyramidial Neurons
     We can have a sort of Pyramidial neurons with distal
     and proximal dentries. How this works is simple. The 
     neuron will have its internal shift register, which will
     be shift once per iteration. It starts off at zero, and
     can be moved to be more negative or positive via the
     influnence of the distal connections.

     The activation function would be the same, but the 
     polarization would take part in the normal activation,
     causing it to activat more quickly if positively poalrized,
     and less if negatively polarized.

     There would be a specific decay rate for this shift register,
     which would be exponential decay. How do we determine that 
     rate? That in an of itself could possibly be evolved.

**** Neocortex Columns
     I don't know how this will work yet, but we could have a set 
     of neurons "replicated" across a "neocortex sheet". Not sure what
     this would entail, but will have to think about it.

     Colums would have connectivity to other columns, and the entire
     business would be evolved together over time to refine the connections.
     That is to say, we may have to employ a HTM-like evolution to it.

***** HTM-like evolution to colums
      This is another issue which we will need to look at closely.
      Replicating the columns, with some area of variation. Not
      sure how this would work either.

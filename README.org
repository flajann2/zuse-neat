* ZuseNEAT                                                         :TOC_5_gh:
  - [[#overview][Overview]]
    - [[#why-zuse][Why Zuse?]]
  - [[#architectural-notes][Architectural Notes]]
    - [[#populations-and-fluidity][Populations and Fluidity]]
    - [[#overall-layout][Overall Layout]]
    - [[#evaluation-interfaces-to-cppns-etc][Evaluation, Interfaces to CPPNs, etc.]]
      - [[#es-iterated-hyperneat-and-cppns][ES Iterated HyperNEAT and CPPNs]]
      - [[#interfaces-for-other-languages][Interfaces for other languages]]
      - [[#resultant-stand-alone-outputs-not-requiring-the-engine][Resultant stand-alone outputs not requiring the engine]]
      - [[#evaluations-and-gpus][Evaluations and GPUs]]
      - [[#independent-vs-tribal-evolution][Independent vs. Tribal evolution]]
      - [[#hyperneat-allows-us-to-sample-large-vectors-or-matricies][HyperNEAT allows us to sample large vectors or matricies]]
    - [[#critters-and-the-shared-critter-loader][Critters and the Shared Critter Loader]]
  - [[#building-and-installing][Building and Installing]]
    - [[#building-requirements][Building Requirements]]
    - [[#how-to-build][How to build]]

** Overview
   This is a / will be my 2nd implementation of the
   NEAT Algorithm, and this time it's in C++17. My
   prior NEAT implementation was in pure Ruby, and was
   more a proof of concept than anything. ZuseNEAT
   shall be much more powerful.

   I have much planned here, but no promises. You'll
   know when the coolness is implemented.
*** Why Zuse?
    Konrad Zuse invented the world's first digital
    computer. This man never got the recognition he
    deserved. As well, his great work was overshadowed
    by der Weltkrieg To find out more,

    https://www.quora.com/Who-is-the-greatest-person-that-history-has-forgotten/answer/Fred-Mitchell-5?share=c89046d6&srid=pTaaZ

    Today, we have moved far beyond his humble initial
    creation, the Z1, which was entirely
    mechanical. History has almost forgotten him, and
    so I name this NEAT implementation after him in
    honer of his accomplishments.

** Architectural Notes
   RAII principles apply.

*** Populations and Fluidity 
    In the prior implementation of NEAT, a "population"
    was a fixed container of Critters, and all the
    Critters were rigidly evaluated and bred at each
    iteration.

    I wish to move away from that and have a more fluid
    approach, that can and will be more
    Critter-oriented, and more fluid, and will allow
    each Critter to be evaluated on its own
    thread. When it's ready to mate, it will flag a
    mating mode and then "seek" a potential mate based
    on the settings of the hyper parameters. It will
    simply linger until it can find another suitable
    critter. It could do continuous evaluation, and
    simply pause at mating time.

    Also, the more fluid approach will also make it
    easier for Critters to compete with each other!

*** Overall Layout 
    | Class       | Description                                                         |
    |-------------+---------------------------------------------------------------------|
    | Geneotype   | Basic gene                                                          |
    | Phenotype   | Basic expression of the genotype, AST or vector, etc                |
    | Critter     | General unit of evolution, the ANN collection                       |
    | Config      | Hyperparameters                                                     |
    | Coordinator | Fluid mode controller                                               |
    | Reporter    | Reports on progress, performace, etc                                |
    | Gateway     | Coordinator and Synchronizer to other ZNEAT nodes on other machines |
    |             |                                                                     |

*** Evaluation, Interfaces to CPPNs, etc.
    We have decided that the evaluation module shall be
    basically be written in C or C++, or in whatever
    language that can call C with the appropriate
    "callbacks".
    
    Ideally, we pass a vector or matrix or tensor to
    the critter, and in return we get something
    back. The same? Why not.

    So, for example, we could pass in an image as a
    matrix of floats. Maybe we'd get back a matrix of
    floats as well, which might represent the
    sharpening of the image, or some other sort of
    processing or feature extraction. Or in the case of
    sound, we pass in a Foureir vector and get
    something back. Or in the case of text, we pass in
    some sort of word vectors, and get something back
    along those lines, or something completely
    different.

**** ES Iterated HyperNEAT and CPPNs
     We come with a pre-canned suite of CPPNs, but
     allow more to be "wired in" via using a specific
     prescribed contract, thus allowing for
     extensibility.

**** Interfaces for other languages
     We wish to make this as "language-agnostic" as
     possible. If someone wants to use Ruby or Python
     or Erlang or Rust to interface with us, we should
     not care. And so language wrappers would have to
     be written.

**** Resultant stand-alone outputs not requiring the engine
     In RubyNEAT, we emitted stand-alone code. In this
     case, we emit object files generated by the LLVM
     that are linkable, and maybe even .so files that
     can be used with other languages. We can rely,
     perhaps, on CMake to make this go in a
     cross-compiler fashion to target any platform.
     
     As such, we will save either the ASTs or IRs for
     LLVM, and have a specific facility to target some
     platform. This will allow us to run the evolution
     on one type of hardware, and target the successful
     critters to something completely different.

**** Evaluations and GPUs
     We have a bit of an issue with data streams going
     accross the GPU/CPU boundaries, which can result
     in a significant slowdown. On the one hand,
     leveraging a GPU with a thousand cores might be a
     very powerful thing to do, except if most of the
     time is spent transferring data back and forth
     with the host system.

     So it may be that we, in that case, keep data
     requirements light, or put the entire problem
     space onto the GPU so that all computations and
     evaluations take place there. Is this a viable
     option? GPUs are not CPUs, so that approach will
     be rather restrictive.
     
     Better will be CPUs with large number of cores, or
     distributed systems, as in the cloud, etc.

**** Independent vs. Tribal evolution
     We want to be able to support both modalities
     where we evaluate the critters in isolation from
     each other, vs.  evaluating the critters in a
     group, or in pairs or similar sub-groupings.

     Keep in mind that there is no longer any definite
     "population" demarcation as there was with
     RubyNEAT. The population will be more sliding in
     the iterations.
     
**** HyperNEAT allows us to sample large vectors or matricies
     We do not have to have input neurons attached to
     all inputs in a vector, but we could simply take
     groups of local inputs and process them in a
     fashion. This would allow us to have variable
     scale inputs, in the Enhanced Substrate fashion.

*** Critters and the Shared Critter Loader
    Critters are generated as shared libraries that are
    dynamically loaded by shared-critter-loader.

    We need to be able to load Critters (as shared
    libraries) dymacally. The functions in the shared
    library can be called directly. See the docs in the
    URL below.
 
    The underlying C interfaces are dladdr(), dlclose(),
    dlerror(), dlopen(), dlsym(), dlvsym(). All is
    described at:
 
    https://linux.die.net/man/3/dlopen

** Building and Installing
*** Building Requirements
    We use clang 5 or later to leverage the full C++17
    specs, and also so that we can eaisly target
    multiple environments.

    We also are using googletest to run our unit tests
    and the like.

    When built, all executables will be in the root of
    the build directory.

*** How to build
    To build this project (usung Ninja):

    #+begin_src bash
    mkdir build
    cd build && cmake -GNinja .. && ninja -k3 -j8
    #+end_src

    Feel free to adjust the "-j8" parameter to reflect
    the number of cores on your build system. You can
    also leave off the "-GNinja" flag if you wish to
    use make instead.

    So, a "no frills" build would look like:

    #+begin_src bash
    mkdir build
    cd build && cmake .. && make
    #+end_src
